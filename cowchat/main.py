import os
import sys
import openai
import argparse
from dotenv import load_dotenv
from cowpy import cow  # Import cowpy library

# Load environment variables from .env file if it exists
load_dotenv()


# Function to get ChatGPT response
def get_chatgpt_response(prompt, model, max_tokens, temperature):
    """
    Fetches a response from ChatGPT based on the provided prompt and configuration.

    Args:
        prompt (str): The user's input prompt to ChatGPT.
        model (str): The model to use for generating the response (e.g., "gpt-3.5-turbo").
        max_tokens (int): The maximum number of tokens to generate for the response.
        temperature (float): The sampling temperature to use. Higher values like 0.9 make the output more random, while lower values like 0.2 make it more focused and deterministic.

    Yields:
        str: Chunks of the response content as it is generated by ChatGPT.
    """
    try:
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
            temperature=temperature,
            stream=True,  # Enable streaming responses
        )
        for chunk in response:
            delta = chunk["choices"][0]["delta"]
            if "content" in delta:
                yield delta["content"]
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


# Function to display the response using cowpy
def display_with_cowpy(message, cowfile=None):
    """
    Displays a message using the cowpy utility with an optional custom cow template.

    Args:
        message (str): The message to be displayed.
        cowfile (str, optional): The name of the custom cowpy template to use. Defaults to None.

    Raises:
        Exception: If an error occurs while generating the cowpy output, it will be printed to stderr.
    """
    try:
        if cowfile:
            cow_instance = getattr(cow, cowfile.title(), cow.Cowacter)()
        else:
            cow_instance = cow.Cowacter()
        print(cow_instance.milk(message))
    except Exception as e:  # pylint disable=broad-exception-caught
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)


# Main function to parse arguments and run the program
def main():
    parser = argparse.ArgumentParser(description="ChatGPT with Cowpy")
    parser.add_argument("prompt", help="The user prompt for ChatGPT")
    parser.add_argument(
        "--model",
        type=str,
        default="gpt-3.5-turbo",
        choices=["gpt-3.5-turbo", "gpt-4o"],
        help="Choose the ChatGPT model (default: gpt-3.5-turbo)",
    )
    parser.add_argument(
        "--max_tokens",
        type=int,
        default=150,
        help="Max tokens for the ChatGPT response",
    )
    parser.add_argument(
        "--temperature", type=float, default=0.7, help="Temperature for chat generation"
    )
    parser.add_argument("--cowfile", type=str, help="Specify a custom cowpy template")
    parser.add_argument("--version", action="version", version="CowChat 1.0")

    args = parser.parse_args()

    api_key = os.getenv("OPENAI_API_KEY")

    if not api_key:
        print(
            "Error: Please set your OpenAI API key in the environment variable 'OPENAI_API_KEY'",
            file=sys.stderr,
        )
        sys.exit(1)

    openai.api_key = api_key

    # Collect the response stream incrementally
    response_text = ""
    for chunk in get_chatgpt_response(
        args.prompt, args.model, args.max_tokens, args.temperature
    ):
        response_text += chunk

    display_with_cowpy(response_text, args.cowfile)


if __name__ == "__main__":
    main()
